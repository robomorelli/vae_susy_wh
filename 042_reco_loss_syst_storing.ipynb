{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/roberto/anaconda3/envs/exotica_ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/roberto/anaconda3/envs/exotica_ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/roberto/anaconda3/envs/exotica_ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/roberto/anaconda3/envs/exotica_ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/roberto/anaconda3/envs/exotica_ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/roberto/anaconda3/envs/exotica_ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Layer, Add, Multiply\n",
    "from keras.models import Model, Sequential\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mean_squared_error\n",
    "from keras import layers as KL\n",
    "\n",
    "import uproot\n",
    "import numpy\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Layer, LeakyReLU, ReLU\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, BaseLogger, \\\n",
    "TerminateOnNaN, Callback, ModelCheckpoint, LambdaCallback\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.constraints import max_norm\n",
    "# from sklearn.externals.joblib import dump, load\n",
    "\n",
    "from numpy.random import seed\n",
    "import time\n",
    "import re\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import random as rn\n",
    "\n",
    "np.random.seed(42)\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "rn.seed(12345)\n",
    "\n",
    "# import ray\n",
    "# from ray.tune.integration.keras import TuneReporterCallback\n",
    "# from ray import tune\n",
    "# from ray.tune.schedulers import AsyncHyperBandScheduler, ASHAScheduler, PopulationBasedTraining\n",
    "import random\n",
    "import csv\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from config import *\n",
    "from vae_utility import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = 3\n",
    "\n",
    "latent_dim = 3\n",
    "intermediate_dim = 50\n",
    "kernel_max_norm = 500.\n",
    "act_fun = 'relu'\n",
    "weight_KL_loss = 0.6\n",
    "\n",
    "Nf_lognorm = 3\n",
    "Nf_PDgauss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_loss_forVAE(mu, sigma, mu_prior, sigma_prior):\n",
    "    kl_loss = K.tf.multiply(K.square(sigma), K.square(sigma_prior))\n",
    "    kl_loss += K.square(K.tf.divide(mu_prior - mu, sigma_prior))\n",
    "    kl_loss += K.log(K.tf.divide(sigma_prior, sigma)) -1\n",
    "    return 0.5 * K.sum(kl_loss, axis=-1)\n",
    "\n",
    "#######################################################################\n",
    "def RecoProb_forVAE(x, par1, par2, par3, w):\n",
    "\n",
    "    N = 0\n",
    "    nll_loss = 0\n",
    "\n",
    "    if Nf_lognorm != 0:\n",
    "\n",
    "        for i in range(Nf_lognorm):\n",
    "\n",
    "            #Log-Normal distributed variables\n",
    "            mu = par1[:,i:i+1]\n",
    "            sigma = par2[:,i:i+1]\n",
    "            fraction = par3[:,i:i+1]\n",
    "            x_clipped = K.clip(x[:,i:i+1], clip_x_to0, 1e8)\n",
    "            single_NLL = K.tf.where(K.less(x[:,i:i+1], clip_x_to0),\n",
    "                                    -K.log(fraction),\n",
    "                                        -K.log(1-fraction)\n",
    "                                        + K.log(sigma)\n",
    "                                        + K.log(x_clipped)\n",
    "                                        + 0.5*K.square(K.tf.divide(K.log(x_clipped) - mu, sigma)))\n",
    "            nll_loss += K.sum(w[i]*single_NLL, axis=-1)\n",
    "\n",
    "        N += Nf_lognorm\n",
    "\n",
    "    if Nf_PDgauss != 0:\n",
    "\n",
    "        for i in range(N, N+Nf_PDgauss):\n",
    "\n",
    "            mu = par1[:,i:i+1]\n",
    "            sigma = par2[:,i:i+1]\n",
    "            norm_xp = K.tf.divide(x[:,i:i+1] + 0.5 - mu, sigma)\n",
    "            norm_xm = K.tf.divide(x[:,i:i+1] - 0.5 - mu, sigma)\n",
    "            sqrt2 = 1.4142135624\n",
    "            single_LL = 0.5*(K.tf.erf(norm_xp/sqrt2) - K.tf.erf(norm_xm/sqrt2))\n",
    "\n",
    "            norm_0 = K.tf.divide(-0.5 - mu, sigma)\n",
    "            aNorm = 1 + 0.5*(1 + K.tf.erf(norm_0/sqrt2))\n",
    "            single_NLL = -K.log(K.clip(single_LL, 1e-10, 1e40)) -K.log(aNorm)\n",
    "\n",
    "            nll_loss += K.sum(w[i]*single_NLL, axis=-1)\n",
    "\n",
    "    return nll_loss\n",
    "\n",
    "\n",
    "def IndividualRecoProb_forVAE_lognorm_1(x, par1, par2, par3, w):\n",
    "    N = 0\n",
    "    nll_loss = 0\n",
    "\n",
    "    mu = par1[:,:1]\n",
    "    sigma = par2[:,:1]\n",
    "    fraction = par3[:,:1]\n",
    "    x_clipped = K.clip(x[:,:1], clip_x_to0, 1e8)\n",
    "    single_NLL = K.tf.where(K.less(x[:,:1], clip_x_to0),\n",
    "                            -K.log(fraction),\n",
    "                                -K.log(1-fraction)\n",
    "                                + K.log(sigma)\n",
    "                                + K.log(x_clipped)\n",
    "                                + 0.5*K.square(K.tf.divide(K.log(x_clipped) - mu, sigma)))\n",
    "    nll_loss += K.sum(w*single_NLL, axis=-1)\n",
    "\n",
    "    return nll_loss\n",
    "\n",
    "def IndividualRecoProb_forVAE_lognorm_2(x, par1, par2, par3, w):\n",
    "    N = 0\n",
    "    nll_loss = 0\n",
    "\n",
    "    mu = par1[:,1:2]\n",
    "    sigma = par2[:,1:2]\n",
    "    fraction = par3[:,1:2]\n",
    "    x_clipped = K.clip(x[:,1:2], clip_x_to0, 1e8)\n",
    "    single_NLL = K.tf.where(K.less(x[:,1:2], clip_x_to0),\n",
    "                            -K.log(fraction),\n",
    "                                -K.log(1-fraction)\n",
    "                                + K.log(sigma)\n",
    "                                + K.log(x_clipped)\n",
    "                                + 0.5*K.square(K.tf.divide(K.log(x_clipped) - mu, sigma)))\n",
    "    nll_loss += K.sum(w*single_NLL, axis=-1)\n",
    "\n",
    "    return nll_loss\n",
    "\n",
    "def IndividualRecoProb_forVAE_lognorm_3(x, par1, par2, par3, w):\n",
    "    N = 0\n",
    "    nll_loss = 0\n",
    "\n",
    "    mu = par1[:,2:3]\n",
    "    sigma = par2[:,2:3]\n",
    "    fraction = par3[:,2:3]\n",
    "    x_clipped = K.clip(x[:,2:3], clip_x_to0, 1e8)\n",
    "    single_NLL = K.tf.where(K.less(x[:,2:3], clip_x_to0),\n",
    "                            -K.log(fraction),\n",
    "                                -K.log(1-fraction)\n",
    "                                + K.log(sigma)\n",
    "                                + K.log(x_clipped)\n",
    "                                + 0.5*K.square(K.tf.divide(K.log(x_clipped) - mu, sigma)))\n",
    "    nll_loss += K.sum(w*single_NLL, axis=-1)\n",
    "\n",
    "    return nll_loss\n",
    "\n",
    "def IndividualRecoProb_forVAE_lognorm_4(x, par1, par2, par3, w):\n",
    "    N = 0\n",
    "    nll_loss = 0\n",
    "\n",
    "    mu = par1[:,3:4]\n",
    "    sigma = par2[:,3:4]\n",
    "    fraction = par3[:,3:4]\n",
    "    x_clipped = K.clip(x[:,3:4], clip_x_to0, 1e8)\n",
    "    single_NLL = K.tf.where(K.less(x[:,3:4], clip_x_to0),\n",
    "                            -K.log(fraction),\n",
    "                                -K.log(1-fraction)\n",
    "                                + K.log(sigma)\n",
    "                                + K.log(x_clipped)\n",
    "                                + 0.5*K.square(K.tf.divide(K.log(x_clipped) - mu, sigma)))\n",
    "    nll_loss += K.sum(w*single_NLL, axis=-1)\n",
    "\n",
    "    return nll_loss\n",
    "\n",
    "def IndividualRecoProb_forVAE_lognorm_5(x, par1, par2, par3, w):\n",
    "    N = 0\n",
    "    nll_loss = 0\n",
    "\n",
    "    mu = par1[:,4:5]\n",
    "    sigma = par2[:,4:5]\n",
    "    fraction = par3[:,4:5]\n",
    "    x_clipped = K.clip(x[:,4:5], clip_x_to0, 1e8)\n",
    "    single_NLL = K.tf.where(K.less(x[:,4:5], clip_x_to0),\n",
    "                            -K.log(fraction),\n",
    "                                -K.log(1-fraction)\n",
    "                                + K.log(sigma)\n",
    "                                + K.log(x_clipped)\n",
    "                                + 0.5*K.square(K.tf.divide(K.log(x_clipped) - mu, sigma)))\n",
    "    nll_loss += K.sum(w*single_NLL, axis=-1)\n",
    "\n",
    "    return nll_loss\n",
    "\n",
    "\n",
    "def individualRecoProb_forVAE_discrete_6(x, par1, par2, w):\n",
    "    nll_loss = 0\n",
    "\n",
    "    mu = par1[:,5:6]\n",
    "    sigma = par2[:,5:6]\n",
    "    norm_xp = K.tf.divide(x[:,5:6] + 0.5 - mu, sigma)\n",
    "    norm_xm = K.tf.divide(x[:,5:6] - 0.5 - mu, sigma)\n",
    "    sqrt2 = 1.4142135624\n",
    "    single_LL = 0.5*(K.tf.erf(norm_xp/sqrt2) - K.tf.erf(norm_xm/sqrt2))\n",
    "\n",
    "    norm_0 = K.tf.divide(-0.5 - mu, sigma)\n",
    "    aNorm = 1 + 0.5*(1 + K.tf.erf(norm_0/sqrt2))\n",
    "    single_NLL = -K.log(K.clip(single_LL, 1e-10, 1e40)) -K.log(aNorm)\n",
    "\n",
    "    nll_loss += K.sum(w*single_NLL, axis=-1)\n",
    "\n",
    "    return nll_loss\n",
    "\n",
    "\n",
    "def individualRecoProb_forVAE_discrete_7(x, par1, par2, w):\n",
    "    nll_loss = 0\n",
    "\n",
    "    mu = par1[:,6:7]\n",
    "    sigma = par2[:,6:7]\n",
    "    norm_xp = K.tf.divide(x[:,6:7] + 0.5 - mu, sigma)\n",
    "    norm_xm = K.tf.divide(x[:,6:7] - 0.5 - mu, sigma)\n",
    "    sqrt2 = 1.4142135624\n",
    "    single_LL = 0.5*(K.tf.erf(norm_xp/sqrt2) - K.tf.erf(norm_xm/sqrt2))\n",
    "\n",
    "    norm_0 = K.tf.divide(-0.5 - mu, sigma)\n",
    "    aNorm = 1 + 0.5*(1 + K.tf.erf(norm_0/sqrt2))\n",
    "    single_NLL = -K.log(K.clip(single_LL, 1e-10, 1e40)) -K.log(aNorm)\n",
    "\n",
    "    nll_loss += K.sum(w*single_NLL, axis=-1)\n",
    "\n",
    "    return nll_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomKLLossLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomKLLossLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mu, sigma, mu_prior, sigma_prior = inputs\n",
    "        return KL_loss_forVAE(mu, sigma, mu_prior, sigma_prior)\n",
    "\n",
    "\n",
    "class CustomRecoProbLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomRecoProbLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, par1, par2, par3 = inputs\n",
    "        return RecoProb_forVAE(x, par1, par2, par3, w = w)\n",
    "\n",
    "#################################################################################################à\n",
    "class CustomIndividualLogNorLayer_1(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomIndividualLogNorLayer_1, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, par1, par2, par3 = inputs\n",
    "        return IndividualRecoProb_forVAE_lognorm_1(x, par1, par2, par3, w = ind_w[0])\n",
    "\n",
    "class CustomIndividualLogNorLayer_2(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomIndividualLogNorLayer_2, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, par1, par2, par3 = inputs\n",
    "        return IndividualRecoProb_forVAE_lognorm_2(x, par1, par2, par3, w = ind_w[1])\n",
    "\n",
    "\n",
    "class CustomIndividualLogNorLayer_3(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomIndividualLogNorLayer_3, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, par1, par2, par3 = inputs\n",
    "        return IndividualRecoProb_forVAE_lognorm_3(x, par1, par2, par3, w = ind_w[2])\n",
    "\n",
    "class CustomIndividualLogNorLayer_4(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomIndividualLogNorLayer_4, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, par1, par2, par3 = inputs\n",
    "        return IndividualRecoProb_forVAE_lognorm_4(x, par1, par2, par3, w = ind_w[3])\n",
    "\n",
    "class CustomIndividualLogNorLayer_5(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomIndividualLogNorLayer_5, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, par1, par2, par3 = inputs\n",
    "        return IndividualRecoProb_forVAE_lognorm_5(x, par1, par2, par3, w = ind_w[4])\n",
    "\n",
    "#         class CustomIndividualLogNorLayer_6(Layer):\n",
    "#             def __init__(self, **kwargs):\n",
    "#                 self.is_placeholder = True\n",
    "#                 super(CustomIndividualLogNorLayer_6, self).__init__(**kwargs)\n",
    "\n",
    "#             def call(self, inputs):\n",
    "#                 x, par1, par2, par3 = inputs\n",
    "#                 return IndividualRecoProb_forVAE_lognorm_6(x, par1, par2, par3, w = ind_w[5])\n",
    "\n",
    "class CustomIndividualTruGauLayer_6(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomIndividualTruGauLayer_6, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, par1, par2 = inputs\n",
    "        return individualRecoProb_forVAE_discrete_6(x, par1, par2, w = ind_w[5])\n",
    "\n",
    "class CustomIndividualTruGauLayer_7(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomIndividualTruGauLayer_7, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, par1, par2 = inputs\n",
    "        return individualRecoProb_forVAE_discrete_7(x, par1, par2, w = ind_w[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "custom_objects = {\n",
    "    'original_dim': 3,\n",
    "    'latent_dim': latent_dim,\n",
    "    'intermediate_dim': intermediate_dim,\n",
    "    'act_fun': act_fun,\n",
    "    'Nf_lognorm' : Nf_lognorm,\n",
    "    'CustomKLLossLayer' : CustomKLLossLayer,\n",
    "    'CustomRecoProbLayer' : CustomRecoProbLayer,\n",
    "    \n",
    "    'CustomIndividualLogNorLayer_1' : CustomIndividualLogNorLayer_1,\n",
    "    'CustomIndividualLogNorLayer_2' : CustomIndividualLogNorLayer_2,\n",
    "    'CustomIndividualLogNorLayer_3' : CustomIndividualLogNorLayer_3,\n",
    "\n",
    "#     'metric' :metric,\n",
    "    'IdentityLoss' : IdentityLoss\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are loading a vae model vae_1.h5\n",
      "{'met': 5, 'mt': 10, 'mct2': 10}\n"
     ]
    }
   ],
   "source": [
    "weight_KL_loss = 0.6\n",
    "\n",
    "path = 'model_results/model_dependent/multiple_train/650_300/0_1_3_ft_3/w_5_10_10/vae_1.h5'\n",
    "\n",
    "sig_inj = path.split('/')[3]\n",
    "\n",
    "check_vae = path.split('/')[-1]\n",
    "\n",
    "if 'vae' in check_vae:\n",
    "    print('you are loading a vae model {}'.format(check_vae))\n",
    "elif 'autoencoder' in check_vae:\n",
    "    print(' \\x1b[31m pay attention, you are loading autoencoder e not vae \\x1b[0m')\n",
    "\n",
    "with open('/'.join(path.split('/')[:-1]) + '/' + 'comps_dict.pickle', 'rb') as handle:\n",
    "    components_dict = pickle.load(handle)\n",
    "    \n",
    "weights = []\n",
    "for k,v in components_dict.items():\n",
    "    weights.append(v)\n",
    "\n",
    "if 0 in weights:\n",
    "    ind_w = [x if x != 0 else 1 for x in weights]\n",
    "else:\n",
    "    ind_w = weights\n",
    "    \n",
    "# ind_w are used for oputput from 2 to 10 (for the 8 individual features)\n",
    "# w are used fot the KL loss and total recon_loss\n",
    "    \n",
    "w = weights\n",
    "print(components_dict)\n",
    "# print('latent dim', latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no zeros in the weights\n",
      "no zeros in the weights, custom loss component definitions ['met', 'mt', 'mct2']\n",
      "lognorm 3 and pdgauss 0\n"
     ]
    }
   ],
   "source": [
    "infer_component = False #This is automatic inference when all weights in components dicts are == 0\n",
    "if not np.all(w): #if thera are 0's in the weights, infer components from dictionaries\n",
    "    print('some zeros in the weights')\n",
    "    selected_components = []\n",
    "    for k,v in components_dict.items():\n",
    "        if components_dict[k] != 0:\n",
    "            selected_components.append(k)\n",
    "    training_components = selected_components\n",
    "    \n",
    "    if infer_component == True:\n",
    "        selected_idx = [cols_sel.index(component) + 2  for component in selected_components]\n",
    "        print('automatic loss component inference {}'.format(selected_components))\n",
    "    elif not infer_component:\n",
    "        selected_components =['met', 'mt', 'mct2']\n",
    "        selected_idx = [cols_sel.index(component) + 2 for component in selected_components]\n",
    "        print('no automatic inference, custom component loss definition {}'.format(selected_components)) \n",
    "    \n",
    "else: #if w are all != 0 we need to input our selection\n",
    "    \n",
    "    print('no zeros in the weights')\n",
    "    selected_components = []\n",
    "    for k,v in components_dict.items():\n",
    "        if components_dict[k] != 0:\n",
    "            selected_components.append(k)\n",
    "    training_components = selected_components\n",
    "    \n",
    "    if infer_component == True:\n",
    "        selected_idx = [cols_sel.index(component) + 2  for component in selected_components]\n",
    "        cols_sel = selected_components\n",
    "        print('automatic loss component inference {}'.format(selected_components))\n",
    "        \n",
    "    else:    \n",
    "        selected_components = ['met', 'mt', 'mct2']\n",
    "        cols_sel = selected_components\n",
    "        print('no zeros in the weights, custom loss component definitions {}'.format(selected_components)) \n",
    "        selected_idx = [cols_sel.index(component) + 2 for component in selected_components]\n",
    "\n",
    "cols_sel = selected_components\n",
    "Nf_lognorm=0\n",
    "Nf_PDgauss=0\n",
    "for selected in selected_components:\n",
    "    if selected in cols[:-3]:\n",
    "        Nf_lognorm += 1\n",
    "    else:\n",
    "        Nf_PDgauss += 1\n",
    "    \n",
    "print('lognorm {} and pdgauss {}'.format(Nf_lognorm, Nf_PDgauss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded\n",
      "\u001b[31m model: vae_1.h5 trained on non-zero loss: ['met', 'mt', 'mct2'] \u001b[0m \n",
      "\n",
      "\u001b[31m total loss weights [5, 10, 10] and individual loss weight: [5, 10, 10] \u001b[0m \n",
      "\n",
      "selected output indexes [2, 3, 4] (shifetd by two beacuse of model output) for the component sum that is: ['met', 'mt', 'mct2']\n"
     ]
    }
   ],
   "source": [
    "vae = load_model(path, custom_objects=custom_objects)\n",
    "print ('Loaded')\n",
    "print(\"\\x1b[31m model: {} trained on non-zero loss: {} \\x1b[0m \\n\".format(path.split('/')[-1], training_components))\n",
    "print(\"\\x1b[31m total loss weights {} and individual loss weight: {} \\x1b[0m \\n\".format(w,ind_w))\n",
    "# print(\"\\x1b[31m selected component for bump loss {} that is: {} \\x1b[0m \\n\".format(w,ind_w))\n",
    "print('selected output indexes {} (shifetd by two beacuse of model output) for the component sum that is: {}'\\\n",
    "      .format(selected_idx, selected_components))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model dependent on 650_300\n"
     ]
    }
   ],
   "source": [
    "if path.split('/')[1] != 'model_dependent':\n",
    "    \n",
    "    model_dependent = False\n",
    "    \n",
    "    back = np.load(train_val_test + 'background.npy')\n",
    "    train = np.load(train_val_test + 'background_train.npy')\n",
    "    val = np.load(train_val_test + 'background_val.npy')\n",
    "\n",
    "    try:\n",
    "        test = np.load(train_val_test + 'background_test.npy')\n",
    "        split_test = 1/(np.sum(test[:,-1])/(np.sum(back[:,-1])))\n",
    "    except:\n",
    "        print('no test, tacking val')\n",
    "        val = np.load(train_val_test + 'background_val.npy')\n",
    "        sample = 'background_val.npy'\n",
    "\n",
    "    split_train = 1/(np.sum(train[:,-1])/(np.sum(back[:,-1])))\n",
    "    split_val = 1/(np.sum(val[:,-1])/(np.sum(back[:,-1])))\n",
    "    print('model independent test, val reweighting {}, {}'.format(split_test, split_val))\n",
    "\n",
    "    \n",
    "elif path.split('/')[1] == 'model_dependent':\n",
    "    \n",
    "    model_dependent = True\n",
    "    \n",
    "    name_sig = sig_inj\n",
    "\n",
    "    back = np.load(train_val_test_mod_dep + 'bkg_sig_{}_30_30_40/background_sig_{}.npy'\\\n",
    "                   .format(name_sig, name_sig))\n",
    "    train = np.load(train_val_test_mod_dep + 'bkg_sig_{}_30_30_40/background_sig_train_{}.npy'\\\n",
    "                    .format(name_sig, name_sig))\n",
    "    val = np.load(train_val_test_mod_dep + 'bkg_sig_{}_30_30_40/background_sig_val_{}.npy'\\\n",
    "                  .format(name_sig, name_sig))\n",
    "    sig_val = np.load(train_val_test_mod_dep + 'bkg_sig_{}_30_30_40/sig_val_{}.npy'\\\n",
    "                  .format(name_sig, name_sig))\n",
    "    val_bkg_only = np.load(train_val_test_mod_dep + 'bkg_sig_{}_30_30_40/background_val_{}.npy'\\\n",
    "                  .format(name_sig, name_sig))\n",
    "    try:\n",
    "        test = np.load(train_val_test_mod_dep + 'bkg_sig_{}_30_30_40/background_sig_test_{}.npy'\\\n",
    "                  .format(name_sig, name_sig))\n",
    "        test_bkg_only = np.load(train_val_test_mod_dep + 'bkg_sig_{}_30_30_40/background_test_{}.npy'\\\n",
    "                  .format(name_sig, name_sig))\n",
    "        sig_test = np.load(train_val_test_mod_dep + 'bkg_sig_{}_30_30_40/sig_test_{}.npy'\\\n",
    "                  .format(name_sig, name_sig))\n",
    "        split_test = 1/(np.sum(test[:,-1])/(np.sum(back[:,-1])))\n",
    "    except:\n",
    "        print('no test')\n",
    "\n",
    "    split_train = 1/(np.sum(train[:,-1])/(np.sum(back[:,-1])))\n",
    "    split_val = 1/(np.sum(val[:,-1])/(np.sum(back[:,-1])))\n",
    "    split_val_bkg_only = 1/(np.sum(val_bkg_only[:,-1])/(np.sum(back[:,-1])))\n",
    "    \n",
    "    split_test = 1/(np.sum(test[:,-1])/(np.sum(back[:,-1])))\n",
    "    split_test_bkg_only = 1/(np.sum(test_bkg_only[:,-1])/(np.sum(back[:,-1])))\n",
    "    \n",
    "    print('model dependent on {}'.format(sig_inj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # sig_test = '600p0_50p0'\n",
    "# signal_sys_d_name = [os.listdir(numpy_sig_syst_down)[os.listdir(numpy_sig_syst_down).index('275p0_50p0')]]\n",
    "# signal_sys_u_name = [os.listdir(numpy_sig_syst_up)[os.listdir(numpy_sig_syst_up).index('275p0_50p0')]]\n",
    "# signal_sys_d_name == signal_sys_u_name\n",
    "\n",
    "signal_sys_d_name = ['275p0_50p0'\n",
    "                     ,'400p0_150p0','700p0_50p0']\n",
    "signal_sys_u_name  = ['275p0_50p0'\n",
    "                      ,'400p0_150p0','700p0_50p0']\n",
    "# signal_sys_d_name = os.listdir(numpy_sig_syst_down)[:5]\n",
    "# signal_sys_u_name = os.listdir(numpy_sig_syst_up)[:5]\n",
    "\n",
    "if '650p0_300p0' not in signal_sys_d_name:\n",
    "    signal_sys_d_name.append('650p0_300p0')\n",
    "    signal_sys_u_name.append('650p0_300p0')\n",
    "signal_sys_d_name == signal_sys_u_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'numpy_data/signal_sys/down/275p0_50p0/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-786c66063d13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy_sig_syst_down\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'{}/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0msignal_sys_d_name_branches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtemp_name\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'numpy_data/signal_sys/down/275p0_50p0/'"
     ]
    }
   ],
   "source": [
    "start_c = 0\n",
    "end_c = len(cols_sel)\n",
    "\n",
    "reg = 'signal'\n",
    "regs =        [ 'all',    'signal',   'reg_2',   'reg_3']\n",
    "ranges_mbb = [[100,350], [100, 140], [140,250], [250,350]]\n",
    "\n",
    "range_mbb = ranges_mbb[regs.index(reg)]\n",
    "range_mct2 = [100,1000]\n",
    "\n",
    "temp = []\n",
    "temp_name = []\n",
    "\n",
    "signals_d_dict = {}\n",
    "signal_names_d = []\n",
    "nominal_signal_names = []\n",
    "\n",
    "signals_mbb_d_dict = {}\n",
    "sig_mbb_d=[]\n",
    "\n",
    "for sig_fold in signal_sys_d_name:\n",
    "#     print(sig_fold)\n",
    "    \n",
    "    dir = numpy_sig_syst_down + '{}/'.format(sig_fold)\n",
    "    signal_sys_d_name_branches = os.listdir(dir)\n",
    "    temp = []\n",
    "    temp_name= []\n",
    "    sig_mbb_d=[]\n",
    "    \n",
    "    for i, name in enumerate(signal_sys_d_name_branches):\n",
    "        \n",
    "        sig_load = np.load(dir + name)\n",
    "        sig_df = pd.DataFrame(sig_load, columns=cols)\n",
    "        sig_df = sig_df[((sig_df['mbb']>=range_mbb[0])&(sig_df['mbb']<range_mbb[1]))]\n",
    "        sig_df = sig_df[((sig_df['mct2']>=range_mct2[0])&(sig_df['mct2']<range_mct2[1]))]\n",
    "\n",
    "        sig_df = sig_df[((sig_df['mt']>=0)&(sig_df['mt']<1000))]\n",
    "        sig_df = sig_df[((sig_df['met']>=0)&(sig_df['met']<1000))]\n",
    "        sig_df = sig_df[((sig_df['mlb1']>=0)&(sig_df['mlb1']<1000))]\n",
    "        sig_df = sig_df[((sig_df['lep1Pt']>=0)&(sig_df['lep1Pt']<1000))]\n",
    "        \n",
    "        temp.append((sig_df[cols_sel[start_c:end_c] + ['weight']]).values)\n",
    "        sig_mbb_d.append(sig_df['mbb'].values)\n",
    "        \n",
    "        if 'p0' in name:\n",
    "            sys_name =''.join(name.split('Wh_hbb_')[1].split('p0')[0:]).split('.')[0]\n",
    "        else:\n",
    "            sys_name =''.join(name.split('Wh_hbb_')[1].split('p5')[0:]).split('.')[0]\n",
    "            \n",
    "        temp_name.append(sys_name)  \n",
    "        \n",
    "    nominal_sig_name = 'Wh_hbb_'+sig_fold+'_0.npy'\n",
    "\n",
    "    if 'p0' in nominal_sig_name:\n",
    "        nominal_name=''.join(nominal_sig_name.split('Wh_hbb_')[1].split('p0')[0:2])\n",
    "    else:\n",
    "        nominal_name=''.join(nominal_sig_name.split('Wh_hbb_')[1].split('p5')[0:2])\n",
    "            \n",
    "    temp_name.append(nominal_name) \n",
    "    sig_load = np.load(numpy_sig + nominal_sig_name)\n",
    "    \n",
    "    sig_df = pd.DataFrame(sig_load, columns=cols)\n",
    "    sig_df = sig_df[((sig_df['mbb']>=range_mbb[0])&(sig_df['mbb']<range_mbb[1]))]\n",
    "    sig_df = sig_df[((sig_df['mct2']>=range_mct2[0])&(sig_df['mct2']<range_mct2[1]))]\n",
    "    \n",
    "    sig_df = sig_df[((sig_df['mt']>=0)&(sig_df['mt']<1000))]\n",
    "    sig_df = sig_df[((sig_df['met']>=0)&(sig_df['met']<1000))]\n",
    "    sig_df = sig_df[((sig_df['mlb1']>=0)&(sig_df['mlb1']<1000))]\n",
    "    sig_df = sig_df[((sig_df['lep1Pt']>=0)&(sig_df['lep1Pt']<1000))]\n",
    "    \n",
    "    temp.append((sig_df[cols_sel[start_c:end_c] + ['weight']]).values)\n",
    "    sig_mbb_d.append(sig_df['mbb'].values)\n",
    "\n",
    "    signals_d_dict[nominal_name] = temp\n",
    "    signals_mbb_d_dict[nominal_name] = sig_mbb_d\n",
    "    \n",
    "    signal_names_d.append(temp_name)\n",
    "    \n",
    "    # Only the nominal name\n",
    "    nominal_signal_names.append(temp_name[-1])\n",
    "    \n",
    "    if nominal_name == sig_inj:\n",
    "        sig_injected_df = sig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "temp_name = []\n",
    "sig_df_mbb_u=[]\n",
    "signals_u_dict = {}\n",
    "signal_names_u = []\n",
    "nominal_signal_names = []\n",
    "\n",
    "signals_mbb_u_dict = {}\n",
    "sig_mbb_d = []\n",
    "\n",
    "for sig_fold in signal_sys_u_name:\n",
    "#     print(sig_fold)\n",
    "    \n",
    "    dir =  numpy_sig_syst_up + '{}/'.format(sig_fold)\n",
    "    signal_sys_u_name_branches = os.listdir(dir)\n",
    "    temp = []\n",
    "    temp_name= []\n",
    "    sig_mbb_u=[]\n",
    "    \n",
    "    for i, name in enumerate(signal_sys_u_name_branches):\n",
    "\n",
    "        sig_load = np.load(dir + name)\n",
    "        sig_df = pd.DataFrame(sig_load, columns=cols)\n",
    "        sig_df = sig_df[((sig_df['mbb']>=range_mbb[0])&(sig_df['mbb']<range_mbb[1]))]\n",
    "        sig_df = sig_df[((sig_df['mct2']>=range_mct2[0])&(sig_df['mct2']<range_mct2[1]))]\n",
    "\n",
    "        sig_df = sig_df[((sig_df['mt']>=0)&(sig_df['mt']<1000))]\n",
    "        sig_df = sig_df[((sig_df['met']>=0)&(sig_df['met']<1000))]\n",
    "        sig_df = sig_df[((sig_df['mlb1']>=0)&(sig_df['mlb1']<1000))]\n",
    "        sig_df = sig_df[((sig_df['lep1Pt']>=0)&(sig_df['lep1Pt']<1000))]\n",
    "        \n",
    "        temp.append((sig_df[cols_sel[start_c:end_c] + ['weight']]).values)\n",
    "        sig_mbb_u.append(sig_df['mbb'].values)\n",
    "        \n",
    "        if 'p0' in name:\n",
    "            sys_name =''.join(name.split('Wh_hbb_')[1].split('p0')[0:]).split('.')[0]\n",
    "        else:\n",
    "            sys_name =''.join(name.split('Wh_hbb_')[1].split('p5')[0:]).split('.')[0]\n",
    "            \n",
    "        temp_name.append(sys_name)               \n",
    "        \n",
    "    nominal_sig_name = 'Wh_hbb_'+sig_fold+'_0.npy'\n",
    "\n",
    "    if 'p0' in nominal_sig_name:\n",
    "        nominal_name=''.join(nominal_sig_name.split('Wh_hbb_')[1].split('p0')[0:2])\n",
    "    else:\n",
    "        nominal_name=''.join(nominal_sig_name.split('Wh_hbb_')[1].split('p5')[0:2])\n",
    "            \n",
    "    temp_name.append(nominal_name) \n",
    "    \n",
    "    sig_load = np.load(numpy_sig + nominal_sig_name)\n",
    "    sig_df = pd.DataFrame(sig_load, columns=cols)\n",
    "    sig_df = sig_df[((sig_df['mbb']>=range_mbb[0])&(sig_df['mbb']<range_mbb[1]))]\n",
    "    sig_df = sig_df[((sig_df['mct2']>=range_mct2[0])&(sig_df['mct2']<range_mct2[1]))]\n",
    "\n",
    "    sig_df = sig_df[((sig_df['mt']>=0)&(sig_df['mt']<1000))]\n",
    "    sig_df = sig_df[((sig_df['met']>=0)&(sig_df['met']<1000))]\n",
    "    sig_df = sig_df[((sig_df['mlb1']>=0)&(sig_df['mlb1']<1000))]\n",
    "    sig_df = sig_df[((sig_df['lep1Pt']>=0)&(sig_df['lep1Pt']<1000))]\n",
    "    \n",
    "    temp.append((sig_df[cols_sel[start_c:end_c] + ['weight']]).values)\n",
    "    sig_mbb_u.append(sig_df['mbb'].values)\n",
    "\n",
    "    signals_u_dict[nominal_name] = temp\n",
    "    signals_mbb_u_dict[nominal_name] = sig_mbb_u\n",
    "    \n",
    "    signal_names_u.append(temp_name)\n",
    "    #Only the nominal\n",
    "    nominal_signal_names.append(temp_name[-1])\n",
    "    \n",
    "    if nominal_name == sig_inj:\n",
    "        sig_injected_df = sig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sig factor: 1\n",
      "2\n",
      "3\n",
      "4\n",
      "2\n",
      "3\n",
      "4\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "start_c = 0\n",
    "end_c = len(cols_sel)\n",
    "\n",
    "reg = 'signal'\n",
    "regs =   [ 'all',    'signal',   'reg_2',   'reg_3']\n",
    "ranges_mbb = [[100,350], [100, 140], [140,250], [250,350]]\n",
    "\n",
    "range_mbb = ranges_mbb[regs.index(reg)]\n",
    "range_mct2 = [100,1000]\n",
    "\n",
    "val_pre_cut_df = pd.DataFrame(val, columns=cols)\n",
    "val_df = pd.DataFrame(val, columns=cols)\n",
    "val_df = val_df[((val_df['mbb']>=range_mbb[0])&(val_df['mbb']<range_mbb[1]))]\n",
    "val_df = val_df[((val_df['mct2']>=range_mct2[0])&(val_df['mct2']<range_mct2[1]))]\n",
    "\n",
    "val_df = val_df[((val_df['mt']>=0)&(val_df['mt']<1000))]\n",
    "val_df = val_df[((val_df['met']>=0)&(val_df['met']<1000))]\n",
    "val_df = val_df[((val_df['mlb1']>=0)&(val_df['mlb1']<1000))]\n",
    "val_df = val_df[((val_df['lep1Pt']>=0)&(val_df['lep1Pt']<1000))]\n",
    "sample_v = (val_df[cols_sel[start_c:end_c] + ['weight']]).values\n",
    "\n",
    "out_bkg_v = vae.predict(sample_v[:,:-1], batch_size=2000)\n",
    "loss_bkg_v = np.column_stack((weight_KL_loss*out_bkg_v[0]+out_bkg_v[1], \n",
    "                                  out_bkg_v[1], out_bkg_v[0])).T\n",
    "if model_dependent:\n",
    "\n",
    "    val_df_b = pd.DataFrame(val_bkg_only, columns=cols)\n",
    "    val_df_b = val_df_b[((val_df_b['mbb']>=range_mbb[0])&(val_df_b['mbb']<range_mbb[1]))]\n",
    "    val_df_b = val_df_b[((val_df_b['mct2']>=range_mct2[0])&(val_df_b['mct2']<range_mct2[1]))]\n",
    "\n",
    "    val_df_b = val_df_b[((val_df_b['mt']>=0)&(val_df_b['mt']<1000))]\n",
    "    val_df_b = val_df_b[((val_df_b['met']>=0)&(val_df_b['met']<1000))]\n",
    "    val_df_b = val_df_b[((val_df_b['mlb1']>=0)&(val_df_b['mlb1']<1000))]\n",
    "    val_df_b = val_df_b[((val_df_b['lep1Pt']>=0)&(val_df_b['lep1Pt']<1000))]\n",
    "    sample_v_b = (val_df_b[cols_sel[start_c:end_c] + ['weight']]).values\n",
    "\n",
    "    out_bkg_only = vae.predict(sample_v_b[:,:-1], batch_size=2000)\n",
    "    loss_bkg_only = np.column_stack((weight_KL_loss*out_bkg_only[0]+out_bkg_only[1], \n",
    "                                  out_bkg_only[1], out_bkg_only[0])).T\n",
    "    \n",
    "############################################# TEST #############################################\n",
    "test_df = pd.DataFrame(test, columns=cols)\n",
    "test_df = test_df[((test_df['mbb']>=range_mbb[0])&(test_df['mbb']<range_mbb[1]))]\n",
    "test_df = test_df[((test_df['mct2']>=range_mct2[0])&(test_df['mct2']<range_mct2[1]))]\n",
    "\n",
    "test_df = test_df[((test_df['mt']>=0)&(test_df['mt']<1000))]\n",
    "test_df = test_df[((test_df['met']>=0)&(test_df['met']<1000))]\n",
    "test_df = test_df[((test_df['mlb1']>=0)&(test_df['mlb1']<1000))]\n",
    "test_df = test_df[((test_df['lep1Pt']>=0)&(test_df['lep1Pt']<1000))]\n",
    "sample_t = (test_df[cols_sel[start_c:end_c] + ['weight']]).values\n",
    "\n",
    "out_bkg_t = vae.predict(sample_t[:,:-1], batch_size=2000)\n",
    "loss_bkg_t = np.column_stack((weight_KL_loss*out_bkg_t[0]+out_bkg_t[1], \n",
    "                                  out_bkg_t[1], out_bkg_t[0])).T\n",
    "if model_dependent:\n",
    "\n",
    "    test_df_b = pd.DataFrame(test_bkg_only, columns=cols)\n",
    "    test_df_b = test_df_b[((test_df_b['mbb']>=range_mbb[0])&(test_df_b['mbb']<range_mbb[1]))]\n",
    "    test_df_b = test_df_b[((test_df_b['mct2']>=range_mct2[0])&(test_df_b['mct2']<range_mct2[1]))]\n",
    "\n",
    "    test_df_b = test_df_b[((test_df_b['mt']>=0)&(test_df_b['mt']<1000))]\n",
    "    test_df_b = test_df_b[((test_df_b['met']>=0)&(test_df_b['met']<1000))]\n",
    "    test_df_b = test_df_b[((test_df_b['mlb1']>=0)&(test_df_b['mlb1']<1000))]\n",
    "    test_df_b = test_df_b[((test_df_b['lep1Pt']>=0)&(test_df_b['lep1Pt']<1000))]\n",
    "    sample_t_b = (test_df_b[cols_sel[start_c:end_c] + ['weight']]).values\n",
    "\n",
    "    out_bkg_only = vae.predict(sample_t_b[:,:-1], batch_size=2000)\n",
    "    loss_bkg_only = np.column_stack((weight_KL_loss*out_bkg_only[0]+out_bkg_only[1], \n",
    "                                  out_bkg_only[1], out_bkg_only[0])).T\n",
    "        \n",
    "\n",
    "sig_factor = 1\n",
    "print('sig factor: {}'.format(sig_factor))\n",
    "\n",
    "for i, sel in enumerate(selected_idx):\n",
    "    print(sel)\n",
    "    if i == 0:\n",
    "        all_components_bkg = out_bkg_v[sel]\n",
    "        all_components_bkg = np.reshape(all_components_bkg, (len(all_components_bkg),1))\n",
    "    else:\n",
    "        all_components_bkg = np.concatenate((all_components_bkg, np.reshape(out_bkg_v[sel]\n",
    "                                        , (len(out_bkg_v[sel]),1))), axis = 1)\n",
    "        \n",
    "    bump_bkg_loss_v=np.sum(all_components_bkg, axis=1)\n",
    "    \n",
    "for i, sel in enumerate(selected_idx):\n",
    "    print(sel)\n",
    "    if i == 0:\n",
    "        all_components_bkg = out_bkg_t[sel]\n",
    "        all_components_bkg = np.reshape(all_components_bkg, (len(all_components_bkg),1))\n",
    "    else:\n",
    "        all_components_bkg = np.concatenate((all_components_bkg, np.reshape(out_bkg_t[sel]\n",
    "                                        , (len(out_bkg_t[sel]),1))), axis = 1)\n",
    "        \n",
    "    bump_bkg_loss_t=np.sum(all_components_bkg, axis=1)\n",
    "    \n",
    "for i, sel in enumerate(selected_idx):\n",
    "    print(sel)\n",
    "    if i == 0:\n",
    "        all_components_bkg = out_bkg_only[sel]\n",
    "        all_components_bkg = np.reshape(all_components_bkg, (len(all_components_bkg),1))\n",
    "    else:\n",
    "        all_components_bkg = np.concatenate((all_components_bkg, np.reshape(out_bkg_only[sel]\n",
    "                                        , (len(out_bkg_only[sel]),1))), axis = 1)\n",
    "        \n",
    "    bump_bkg_only_loss_t=np.sum(all_components_bkg, axis=1)\n",
    "    \n",
    "# ATTAch the loss to the dataframe\n",
    "\n",
    "val_df['loss'] = bump_bkg_loss_v\n",
    "test_df['loss'] = bump_bkg_loss_t\n",
    "test_df_b['loss'] = bump_bkg_only_loss_t\n",
    "\n",
    "# val_df = val_df.reset_index().copy()\n",
    "# sig_in_val= pd.merge(sig_injected_df,val_df, how='inner', on=cols_sel[start_c:end_c])\n",
    "# remove = list(sig_in_val['index'])\n",
    "# val_df_b = (val_df[~val_df['index'].isin(remove)]).copy()\n",
    "\n",
    "# test_df = test_df.reset_index().copy()\n",
    "# ### WE NEDD THIS because of the cut on the signal## or apply same cut to ongly bkg (sample_t_b for example)\n",
    "# sig_in_test= pd.merge(sig_injected_df,test_df, how='inner', on=cols_sel[start_c:end_c])\n",
    "# remove = list(sig_in_test['index'])\n",
    "\n",
    "# test_df_b_2 = (test_df[~test_df['index'].isin(remove)]).copy()\n",
    "\n",
    "# test_df = test_df.copy()\n",
    "# test_df_b = test_df_b.copy()\n",
    "# sig_in_test = sig_in_test.copy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmlowei = False\n",
    "\n",
    "# if not rmlowei:\n",
    "#     val_df = val_df[val_df['weight']>val_df['weight'].min()]\n",
    "#     test_df = test_df[test_df['weight']>test_df['weight'].min()]\n",
    "#     val_df_b = val_df_b[val_df_b['weight']>val_df_b['weight'].min()]\n",
    "#     test_df_b = test_df_b[test_df_b['weight']>test_df_b['weight'].min()]\n",
    "    \n",
    "#     rmlowei=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_val = 'test'\n",
    "bkg_only = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n",
      "275_50down\n",
      "275_50up\n",
      "400_150down\n",
      "400_150up\n",
      "700_50down\n",
      "700_50up\n",
      "650_300down\n",
      "650_300up\n"
     ]
    }
   ],
   "source": [
    "regions_edge = np.array([100. , 112.5, 125. , 137.5, 150. , 162.5, 175. , 187.5, 200. ,\n",
    "        212.5, 225. , 237.5, 250. , 262.5, 275. , 287.5, 300. , 312.5,\n",
    "        325. , 337.5, 350. ])\n",
    "\n",
    "\n",
    "if (sample_val == 'val') & (bkg_only):\n",
    "    reweight = split_val_bkg_only\n",
    "    dataset = (val_df_b[cols_sel[start_c:end_c] + ['weight']]).values\n",
    "    mbb_bkg = val_df_b['mbb'].values\n",
    "    \n",
    "elif (sample_val == 'val') & (not bkg_only):\n",
    "    reweight = split_val\n",
    "    dataset = (val_df[cols_sel[start_c:end_c] + ['weight']]).values\n",
    "    mbb_bkg = val_df['mbb'].values\n",
    "\n",
    "elif (sample_val == 'test') & (bkg_only):\n",
    "    reweight = split_test_bkg_only\n",
    "    dataset = (test_df_b[cols_sel[start_c:end_c] + ['weight']]).values\n",
    "    mbb_bkg = test_df_b['mbb'].values\n",
    "    print('test bkg only')\n",
    "    \n",
    "elif (sample_val == 'test') & (not bkg_only):\n",
    "    reweight = split_test\n",
    "    dataset = (test_df[cols_sel[start_c:end_c] + ['weight']]).values\n",
    "    mbb_bkg = test_df['mbb'].values\n",
    "    \n",
    "N_exp = 30\n",
    "\n",
    "# For each nominal signal we want a dictionary where put all the efficiencies \n",
    "# of all the systematics and the nominal\n",
    "\n",
    "maxi_t = 2000\n",
    "recon_loss_dic_d = {}\n",
    "recon_loss_dic_d_mean = {}\n",
    "\n",
    "weight_dict_d = {}\n",
    "mbb_dict_d = {}\n",
    "\n",
    "bkg_d_list = []\n",
    "weight_list_d = []\n",
    "mbb_list_d = []\n",
    "\n",
    "recon_loss_dic_u = {}\n",
    "recon_loss_dic_u_mean = {}\n",
    "\n",
    "weight_dict_u = {}\n",
    "mbb_dict_u = {}\n",
    "\n",
    "bkg_u_list = []\n",
    "bkg_list = []\n",
    "\n",
    "if bkg_only:\n",
    "    bkg_name = 'bkg_only_NoSys'\n",
    "else:\n",
    "    bkg_name = 'bkg_sig_NoSys'\n",
    "    \n",
    "for row in range(N_exp):\n",
    "    \n",
    "    out_bkg = vae.predict(dataset[:,:-1], batch_size=2000)\n",
    "    for i, sel in enumerate(selected_idx):\n",
    "\n",
    "        if i == 0:\n",
    "            all_components_bkg = out_bkg[sel]\n",
    "            all_components_bkg = np.reshape(all_components_bkg, (len(all_components_bkg),1))\n",
    "        else:\n",
    "            all_components_bkg = np.concatenate((all_components_bkg, np.reshape(out_bkg[sel]\n",
    "                                            , (len(out_bkg[sel]),1))), axis = 1)\n",
    "\n",
    "    bump_loss_bkg=np.sum(all_components_bkg, axis=1)\n",
    "    bkg_list.append(bump_loss_bkg)\n",
    "    \n",
    "    for idx_sig, sig_name in enumerate(nominal_signal_names):\n",
    "        print(sig_name + 'down')\n",
    "        if row == 0:\n",
    "            recon_loss_dic_d[sig_name] = {} #dict because we need to append other syst_name dict\n",
    "            weight_dict_d[sig_name] = [] #we only append the list without the syst name (no mean on these)\n",
    "            mbb_dict_d[sig_name] = []\n",
    "\n",
    "        for sig_id, sig in enumerate(signals_d_dict[sig_name]): \n",
    "            if row == 0:\n",
    "                # Here the variable we don't need to mean (take once)\n",
    "                ####################CHOICE on REWEIGHTING#########################################\n",
    "                weight_dict_d[sig_name].append(np.concatenate((dataset[:,-1]*reweight, sig[:,-1])))\n",
    "                mbb_dict_d[sig_name].append(np.concatenate((mbb_bkg, signals_mbb_d_dict[sig_name][sig_id])))\n",
    "                if sig_id + 1 == len(signals_d_dict[sig_name]):\n",
    "#                     recon_loss_dic_d[sig_name][signal_names_d[idx_sig][sig_id] + '_NoSys']=[]\n",
    "                    # WE just skip the nominal signal because we want once and is computed later in the\n",
    "                    # upper systematicas (lastt signal)\n",
    "                    continue\n",
    "                else:\n",
    "                    recon_loss_dic_d[sig_name][signal_names_d[idx_sig][sig_id]]=[]\n",
    "\n",
    "            out_sig = vae.predict(sig[:,:-1], batch_size=2000)\n",
    "\n",
    "            for i, sel in enumerate(selected_idx):\n",
    "                if i == 0:\n",
    "                    all_components_sig_temp = out_sig[sel]\n",
    "                    all_components_sig_temp = np.reshape(all_components_sig_temp\n",
    "                                                         , (len(all_components_sig_temp),1))\n",
    "                else:\n",
    "                    all_components_sig_temp = (np.concatenate((all_components_sig_temp\n",
    "                                                , np.reshape(out_sig[sel]\n",
    "                                                    , (len(out_sig[sel]),1))), axis = 1))\n",
    "            bump_loss_sig = np.sum(all_components_sig_temp, axis = 1)\n",
    "            bkg_sig = np.concatenate((bump_loss_bkg, bump_loss_sig), axis = 0)\n",
    "\n",
    "            if sig_id + 1 == len(signals_d_dict[sig_name]):\n",
    "#                 recon_loss_dic_d[sig_name][signal_names_d[idx_sig][sig_id] + '_NoSys'].append(bkg_sig)\n",
    "                # WE just skip the nominal signal because we want once and is computed later in the\n",
    "                # upper systematicas (lastt signal)\n",
    "                continue\n",
    "            else:\n",
    "                recon_loss_dic_d[sig_name][signal_names_d[idx_sig][sig_id]].append(bkg_sig)\n",
    "                \n",
    "        # UP SYStematics\n",
    "        \n",
    "        print(sig_name + 'up')\n",
    "        if row == 0:\n",
    "            recon_loss_dic_u[sig_name] = {} #dict because we need to append other syst_name dict\n",
    "            weight_dict_u[sig_name] = [] #we only append the list without the syst name (no mean on these)\n",
    "            mbb_dict_u[sig_name] = []\n",
    "                \n",
    "        for sig_id, sig in enumerate(signals_u_dict[sig_name]): \n",
    "            if row == 0:\n",
    "                # Here the variable we don't need to mean (take once)\n",
    "                weight_dict_u[sig_name].append(np.concatenate((dataset[:,-1]*reweight, sig[:,-1])))\n",
    "                mbb_dict_u[sig_name].append(np.concatenate((mbb_bkg, signals_mbb_u_dict[sig_name][sig_id])))\n",
    "                if sig_id + 1 == len(signals_u_dict[sig_name]):\n",
    "                    recon_loss_dic_u[sig_name][signal_names_u[idx_sig][sig_id] + '_NoSys']=[]\n",
    "                else:\n",
    "                    recon_loss_dic_u[sig_name][signal_names_u[idx_sig][sig_id]]=[]\n",
    "\n",
    "            out_sig = vae.predict(sig[:,:-1], batch_size=2000)\n",
    "\n",
    "            for i, sel in enumerate(selected_idx):\n",
    "                if i == 0:\n",
    "                    all_components_sig_temp = out_sig[sel]\n",
    "                    all_components_sig_temp = np.reshape(all_components_sig_temp\n",
    "                                                         , (len(all_components_sig_temp),1))\n",
    "                else:\n",
    "                    all_components_sig_temp = (np.concatenate((all_components_sig_temp\n",
    "                                                , np.reshape(out_sig[sel]\n",
    "                                                    , (len(out_sig[sel]),1))), axis = 1))\n",
    "            bump_loss_sig = np.sum(all_components_sig_temp, axis = 1)\n",
    "            bkg_sig = np.concatenate((bump_loss_bkg, bump_loss_sig), axis = 0)\n",
    "\n",
    "            if sig_id + 1 == len(signals_u_dict[sig_name]):\n",
    "                recon_loss_dic_u[sig_name][signal_names_u[idx_sig][sig_id] + '_NoSys'].append(bkg_sig)\n",
    "            else:\n",
    "                recon_loss_dic_u[sig_name][signal_names_u[idx_sig][sig_id]].append(bkg_sig)\n",
    "                \n",
    "                \n",
    "\n",
    "for idx_sig, sig_name in enumerate(nominal_signal_names):\n",
    "    recon_loss_dic_d_mean[sig_name] = {}\n",
    "    for k,v in recon_loss_dic_d[sig_name].items(): # We don't have the nnoSYs key(skipped)\n",
    "        recon_loss_dic_d_mean[sig_name][k] = np.mean(recon_loss_dic_d[sig_name][k],axis = 0)\n",
    "        \n",
    "    ######################################################################################\n",
    "#     recon_loss_dic_d_mean[sig_name]['{}_bkg'.format(sig_name)] = np.mean(bkg_list, axis=0)        \n",
    "#     weight_dict_d[sig_name].append(dataset[:,-1]*reweight)\n",
    "#     mbb_dict_d[sig_name].append(mbb_bkg)\n",
    "\n",
    "    # We wat only once (later in the upper variations)\n",
    "    ##########################################################################\n",
    "    \n",
    "    recon_loss_dic_d_mean[sig_name]['weight'] = weight_dict_d[sig_name]\n",
    "    recon_loss_dic_d_mean[sig_name]['mbb'] = mbb_dict_d[sig_name]\n",
    "    \n",
    "for idx_sig, sig_name in enumerate(nominal_signal_names):\n",
    "    recon_loss_dic_u_mean[sig_name] = {}\n",
    "    for k,v in recon_loss_dic_u[sig_name].items():\n",
    "        recon_loss_dic_u_mean[sig_name][k] = np.mean(recon_loss_dic_u[sig_name][k],axis = 0)\n",
    "    \n",
    "    #########################################################################################    \n",
    "    recon_loss_dic_u_mean[sig_name]['{}_{}'.format(sig_name,bkg_name)] = np.mean(bkg_list, axis=0)    \n",
    "    weight_dict_u[sig_name].append(dataset[:,-1]*reweight)\n",
    "    mbb_dict_u[sig_name].append(mbb_bkg)\n",
    "    #########################################################################################\n",
    "    \n",
    "    recon_loss_dic_u_mean[sig_name]['weight'] = weight_dict_u[sig_name]\n",
    "    recon_loss_dic_u_mean[sig_name]['mbb'] = mbb_dict_u[sig_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regions_edge = np.array([100. , 112.5, 125. , 137.5, 150. , 162.5, 175. , 187.5, 200. ,\n",
    "#         212.5, 225. , 237.5, 250. , 262.5, 275. , 287.5, 300. , 312.5,\n",
    "#         325. , 337.5, 350. ])\n",
    "\n",
    "\n",
    "# if (sample_val == 'val') & (bkg_only):\n",
    "#     reweight = split_val_bkg_only\n",
    "#     dataset = (val_df_b[cols_sel[start_c:end_c] + ['weight']]).values\n",
    "#     mbb_bkg = val_df_b['mbb'].values\n",
    "    \n",
    "# elif (sample_val == 'val') & (not bkg_only):\n",
    "#     reweight = split_val\n",
    "#     dataset = (val_df[cols_sel[start_c:end_c] + ['weight']]).values\n",
    "#     mbb_bkg = val_df['mbb'].values\n",
    "\n",
    "# elif (sample_val == 'test') & (bkg_only):\n",
    "#     reweight = split_test_bkg_only\n",
    "#     dataset = (test_df_b[cols_sel[start_c:end_c] + ['weight']]).values\n",
    "#     mbb_bkg = test_df_b['mbb'].values\n",
    "#     print('test bkg only')\n",
    "    \n",
    "# elif (sample_val == 'test') & (not bkg_only):\n",
    "#     reweight = split_test\n",
    "#     dataset = (test_df[cols_sel[start_c:end_c] + ['weight']]).values\n",
    "#     mbb_bkg = test_df['mbb'].values\n",
    "    \n",
    "# N_exp = 30\n",
    "\n",
    "# # For each nominal signal we want a dictionary where put all the efficiencies \n",
    "# # of all the systematics and the nominal\n",
    "\n",
    "# maxi_t = 2000\n",
    "# recon_loss_dic_d = {}\n",
    "# recon_loss_dic_d_mean = {}\n",
    "\n",
    "# weight_dict_d = {}\n",
    "# mbb_dict_d = {}\n",
    "\n",
    "# bkg_d_list = []\n",
    "# weight_list_d = []\n",
    "# mbb_list_d = []\n",
    "# for row in range(N_exp):\n",
    "    \n",
    "#     out_bkg = vae.predict(dataset[:,:-1], batch_size=2000)\n",
    "#     for i, sel in enumerate(selected_idx):\n",
    "\n",
    "#         if i == 0:\n",
    "#             all_components_bkg = out_bkg[sel]\n",
    "#             all_components_bkg = np.reshape(all_components_bkg, (len(all_components_bkg),1))\n",
    "#         else:\n",
    "#             all_components_bkg = np.concatenate((all_components_bkg, np.reshape(out_bkg[sel]\n",
    "#                                             , (len(out_bkg[sel]),1))), axis = 1)\n",
    "\n",
    "#     bump_loss_bkg=np.sum(all_components_bkg, axis=1)\n",
    "#     bkg_d_list.append(bump_loss_bkg)\n",
    "    \n",
    "#     for idx_sig, sig_name in enumerate(nominal_signal_names):\n",
    "#         print(sig_name)\n",
    "#         if row == 0:\n",
    "#             recon_loss_dic_d[sig_name] = {} #dict because we need to append other syst_name dict\n",
    "#             weight_dict_d[sig_name] = [] #we only append the list without the syst name (no mean on these)\n",
    "#             mbb_dict_d[sig_name] = []\n",
    "\n",
    "#         for sig_id, sig in enumerate(signals_d_dict[sig_name]): \n",
    "#             if row == 0:\n",
    "#                 # Here the variable we don't need to mean (take once)\n",
    "# #                 weight_list_d.append(np.concatenate((dataset[:,-1]*reweight, sig[:,-1])))\n",
    "# #                 mbb_list_d.append(np.concatenate((mbb_bkg, signals_mbb_d_dict[sig_name][sig_id])))\n",
    "#                 weight_dict_d[sig_name].append(np.concatenate((dataset[:,-1]*reweight, sig[:,-1])))\n",
    "#                 mbb_dict_d[sig_name].append(np.concatenate((mbb_bkg, signals_mbb_d_dict[sig_name][sig_id])))\n",
    "#                 if sig_id + 1 == len(signals_d_dict[sig_name]):\n",
    "#                     recon_loss_dic_d[sig_name][signal_names_d[idx_sig][sig_id] + '_NoSys__1down']=[]\n",
    "#                 else:\n",
    "#                     recon_loss_dic_d[sig_name][signal_names_d[idx_sig][sig_id]]=[]\n",
    "\n",
    "#             out_sig = vae.predict(sig[:,:-1], batch_size=2000)\n",
    "\n",
    "#             for i, sel in enumerate(selected_idx):\n",
    "#                 if i == 0:\n",
    "#                     all_components_sig_temp = out_sig[sel]\n",
    "#                     all_components_sig_temp = np.reshape(all_components_sig_temp\n",
    "#                                                          , (len(all_components_sig_temp),1))\n",
    "#                 else:\n",
    "#                     all_components_sig_temp = (np.concatenate((all_components_sig_temp\n",
    "#                                                 , np.reshape(out_sig[sel]\n",
    "#                                                     , (len(out_sig[sel]),1))), axis = 1))\n",
    "#             bump_loss_sig = np.sum(all_components_sig_temp, axis = 1)\n",
    "#             bkg_sig = np.concatenate((bump_loss_bkg, bump_loss_sig), axis = 0)\n",
    "\n",
    "#             if sig_id + 1 == len(signals_d_dict[sig_name]):\n",
    "#                 recon_loss_dic_d[sig_name][signal_names_d[idx_sig][sig_id] + '_NoSys__1down'].append(bkg_sig)\n",
    "#             else:\n",
    "#                 recon_loss_dic_d[sig_name][signal_names_d[idx_sig][sig_id]].append(bkg_sig)\n",
    "\n",
    "# for idx_sig, sig_name in enumerate(nominal_signal_names):\n",
    "#     recon_loss_dic_d_mean[sig_name] = {}\n",
    "#     for k,v in recon_loss_dic_d[sig_name].items():\n",
    "#         recon_loss_dic_d_mean[sig_name][k] = np.mean(recon_loss_dic_d[sig_name][k],axis = 0)\n",
    "#     recon_loss_dic_d_mean[sig_name]['{}_bkg__1down'.format(sig_name)] = np.mean(bkg_d_list, axis=0) \n",
    "        \n",
    "#     weight_dict_d[sig_name].append(dataset[:,-1]*reweight)\n",
    "#     mbb_dict_d[sig_name].append(mbb_bkg)\n",
    "#     recon_loss_dic_d_mean[sig_name]['weight'] = weight_dict_d[sig_name]\n",
    "#     recon_loss_dic_d_mean[sig_name]['mbb'] = mbb_dict_d[sig_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regions_edge = np.array([100. , 112.5, 125. , 137.5, 150. , 162.5, 175. , 187.5, 200. ,\n",
    "#         212.5, 225. , 237.5, 250. , 262.5, 275. , 287.5, 300. , 312.5,\n",
    "#         325. , 337.5, 350. ])\n",
    "\n",
    "\n",
    "# if (sample_val == 'val') & (bkg_only):\n",
    "#     reweight = split_val_bkg_only\n",
    "#     dataset = (val_df_b[cols_sel[start_c:end_c] + ['weight']]).values\n",
    "#     mbb_bkg = val_df_b['mbb'].values\n",
    "    \n",
    "# elif (sample_val == 'val') & (not bkg_only):\n",
    "#     reweight = split_val\n",
    "#     dataset = (val_df[cols_sel[start_c:end_c] + ['weight']]).values\n",
    "#     mbb_bkg = val_df['mbb'].values\n",
    "\n",
    "# elif (sample_val == 'test') & (bkg_only):\n",
    "#     reweight = split_test_bkg_only\n",
    "#     dataset = (test_df_b[cols_sel[start_c:end_c] + ['weight']]).values\n",
    "#     mbb_bkg = test_df_b['mbb'].values\n",
    "#     print('test bkg only')\n",
    "    \n",
    "# elif (sample_val == 'test') & (not bkg_only):\n",
    "#     reweight = split_test\n",
    "#     dataset = (test_df[cols_sel[start_c:end_c] + ['weight']]).values\n",
    "#     mbb_bkg = test_df['mbb'].values\n",
    "    \n",
    "# N_exp = 30\n",
    "\n",
    "# # For each nominal signal we want a dictionary where put all the efficiencies \n",
    "# # of all the systematics and the nominal\n",
    "\n",
    "# maxi_t = 2000\n",
    "# recon_loss_dic_u = {}\n",
    "# recon_loss_dic_u_mean = {}\n",
    "\n",
    "# weight_dict_u = {}\n",
    "# mbb_dict_u = {}\n",
    "\n",
    "# bkg_u_list = []\n",
    "# weight_list_u = []\n",
    "# mbb_list_u = []\n",
    "# for row in range(N_exp):\n",
    "    \n",
    "#     out_bkg = vae.predict(dataset[:,:-1], batch_size=2000)\n",
    "#     for i, sel in enumerate(selected_idx):\n",
    "\n",
    "#         if i == 0:\n",
    "#             all_components_bkg = out_bkg[sel]\n",
    "#             all_components_bkg = np.reshape(all_components_bkg, (len(all_components_bkg),1))\n",
    "#         else:\n",
    "#             all_components_bkg = np.concatenate((all_components_bkg, np.reshape(out_bkg[sel]\n",
    "#                                             , (len(out_bkg[sel]),1))), axis = 1)\n",
    "\n",
    "#     bump_loss_bkg=np.sum(all_components_bkg, axis=1)\n",
    "#     bkg_u_list.append(bump_loss_bkg)\n",
    "    \n",
    "#     for idx_sig, sig_name in enumerate(nominal_signal_names):\n",
    "#         print(sig_name)\n",
    "#         if row == 0:\n",
    "#             recon_loss_dic_u[sig_name] = {} #dict because we need to append other syst_name dict\n",
    "#             weight_dict_u[sig_name] = [] #we only append the list without the syst name (no mean on these)\n",
    "#             mbb_dict_u[sig_name] = []\n",
    "\n",
    "#         for sig_id, sig in enumerate(signals_u_dict[sig_name]): \n",
    "#             if row == 0:\n",
    "#                 # Here the variable we don't need to mean (take once)\n",
    "# #                 weight_list_d.append(np.concatenate((dataset[:,-1]*reweight, sig[:,-1])))\n",
    "# #                 mbb_list_d.append(np.concatenate((mbb_bkg, signals_mbb_d_dict[sig_name][sig_id])))\n",
    "#                 weight_dict_u[sig_name].append(np.concatenate((dataset[:,-1]*reweight, sig[:,-1])))\n",
    "#                 mbb_dict_u[sig_name].append(np.concatenate((mbb_bkg, signals_mbb_u_dict[sig_name][sig_id])))\n",
    "#                 if sig_id + 1 == len(signals_u_dict[sig_name]):\n",
    "#                     recon_loss_dic_u[sig_name][signal_names_u[idx_sig][sig_id] + '_NoSys__1up']=[]\n",
    "#                 else:\n",
    "#                     recon_loss_dic_u[sig_name][signal_names_u[idx_sig][sig_id]]=[]\n",
    "\n",
    "#             out_sig = vae.predict(sig[:,:-1], batch_size=2000)\n",
    "\n",
    "#             for i, sel in enumerate(selected_idx):\n",
    "#                 if i == 0:\n",
    "#                     all_components_sig_temp = out_sig[sel]\n",
    "#                     all_components_sig_temp = np.reshape(all_components_sig_temp\n",
    "#                                                          , (len(all_components_sig_temp),1))\n",
    "#                 else:\n",
    "#                     all_components_sig_temp = (np.concatenate((all_components_sig_temp\n",
    "#                                                 , np.reshape(out_sig[sel]\n",
    "#                                                     , (len(out_sig[sel]),1))), axis = 1))\n",
    "#             bump_loss_sig = np.sum(all_components_sig_temp, axis = 1)\n",
    "#             bkg_sig = np.concatenate((bump_loss_bkg, bump_loss_sig), axis = 0)\n",
    "\n",
    "#             if sig_id + 1 == len(signals_u_dict[sig_name]):\n",
    "#                 recon_loss_dic_u[sig_name][signal_names_u[idx_sig][sig_id] + '_NoSys__1up'].append(bkg_sig)\n",
    "#             else:\n",
    "#                 recon_loss_dic_u[sig_name][signal_names_u[idx_sig][sig_id]].append(bkg_sig)\n",
    "\n",
    "# for idx_sig, sig_name in enumerate(nominal_signal_names):\n",
    "#     recon_loss_dic_u_mean[sig_name] = {}\n",
    "#     for k,v in recon_loss_dic_u[sig_name].items():\n",
    "#         recon_loss_dic_u_mean[sig_name][k] = np.mean(recon_loss_dic_u[sig_name][k],axis = 0)\n",
    "#     recon_loss_dic_u_mean[sig_name]['{}_bkg__1up'.format(sig_name)] = np.mean(bkg_u_list, axis=0) \n",
    "        \n",
    "#     weight_dict_u[sig_name].append(dataset[:,-1]*reweight)\n",
    "#     mbb_dict_u[sig_name].append(mbb_bkg)\n",
    "#     recon_loss_dic_u_mean[sig_name]['weight'] = weight_dict_u[sig_name]\n",
    "#     recon_loss_dic_u_mean[sig_name]['mbb'] = mbb_dict_u[sig_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_results/stored_results/650_300_train_injection_mod_dependent/\n"
     ]
    }
   ],
   "source": [
    "if bkg_only:\n",
    "    type_mod = 'model_independent'\n",
    "    save_path = 'root_results/stored_results/{}_train_injection_mod_independent/'.format(sig_inj)\n",
    "elif not bkg_only:\n",
    "    save_path = 'root_results/stored_results/{}_train_injection_mod_dependent/'.format(sig_inj)\n",
    "    type_mod = 'model_dependent'\n",
    "print(save_path)\n",
    "try:\n",
    "    os.makedirs(save_path)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass on 275_50_NoSys\n",
      "pass on 275_50_bkg_sig_NoSys\n",
      "pass on 400_150_NoSys\n",
      "pass on 400_150_bkg_sig_NoSys\n",
      "pass on 700_50_NoSys\n",
      "pass on 700_50_bkg_sig_NoSys\n",
      "pass on 650_300_NoSys\n",
      "pass on 650_300_bkg_sig_NoSys\n"
     ]
    }
   ],
   "source": [
    "b1 = uproot.newbranch(\"f8\", compression=uproot.ZLIB(5))\n",
    "b2 = uproot.newbranch(\"f8\", compression=uproot.LZMA(4))\n",
    "b3 = uproot.newbranch(\"f8\", compression=uproot.LZMA(4))\n",
    "\n",
    "\n",
    "branchdict = {\"mbb\": b1, \"weight\": b2, \"reco_loss\":b3}\n",
    "tree = uproot.newtree(branchdict, compression=uproot.LZ4(4))\n",
    "with uproot.recreate(save_path+'Wh_results_{}_to_{}.root'.format(nominal_signal_names[0],\n",
    "                                                                nominal_signal_names[-1])\n",
    "                     , compression=uproot.LZMA(5)) as f:\n",
    "    \n",
    "    for sig_id, sig_name in enumerate(nominal_signal_names):\n",
    "\n",
    "        for ix, (k,v) in enumerate(recon_loss_dic_u_mean[sig_name].items()):\n",
    "\n",
    "            if (k == 'weight') | (k == 'mbb'):\n",
    "                continue\n",
    "\n",
    "            f[k] = tree\n",
    "            f[k].extend({\"mbb\": numpy.array(list(recon_loss_dic_u_mean[sig_name]['mbb'][ix])), \n",
    "                        \"weight\": numpy.array(list(recon_loss_dic_u_mean[sig_name]['weight'][ix])), \n",
    "                        \"reco_loss\": numpy.array(list(v))}) \n",
    "\n",
    "            try:\n",
    "                k_d = re.split(r'\\d+up', k)[0] + '{}down'.format(re.split(r'__', k)[1][0])\n",
    "\n",
    "                v = recon_loss_dic_d_mean[sig_name][k_d]\n",
    "                ix_d = list(recon_loss_dic_d_mean[sig_name]).index(k_d)\n",
    "\n",
    "                f[k_d] = tree\n",
    "                f[k_d].extend({\"mbb\": numpy.array(list(recon_loss_dic_d_mean[sig_name]['mbb'][ix_d])), \n",
    "                        \"weight\": numpy.array(list(recon_loss_dic_d_mean[sig_name]['weight'][ix_d])), \n",
    "                        \"reco_loss\": numpy.array(list(v))})  \n",
    "            except:\n",
    "                try:\n",
    "                    k_d = re.split(r'__up', k)[0] + '__down'\n",
    "                    v = recon_loss_dic_d_mean[sig_name][k_d]\n",
    "                    ix_d = list(recon_loss_dic_d_mean[sig_name]).index(k_d)\n",
    "\n",
    "                    f[k_d] = tree\n",
    "                    f[k_d].extend({\"mbb\": numpy.array(list(recon_loss_dic_d_mean[sig_name]['mbb'][ix_d])), \n",
    "                            \"weight\": numpy.array(list(recon_loss_dic_d_mean[sig_name]['weight'][ix_d])), \n",
    "                            \"reco_loss\": numpy.array(list(v))})                  \n",
    "                except:\n",
    "                    \n",
    "                    print('pass on {}'.format(k))\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'650_300_MUON_MS__1up;1',\n",
       " b'650_300_MUON_MS__1down;1',\n",
       " b'650_300_MUON_SAGITTA_RESBIAS__1up;1',\n",
       " b'650_300_MUON_SAGITTA_RESBIAS__1down;1',\n",
       " b'650_300_MUON_SAGITTA_RHO__1up;1',\n",
       " b'650_300_MUON_SAGITTA_RHO__1down;1',\n",
       " b'650_300_MUON_SCALE__1up;1',\n",
       " b'650_300_MUON_SCALE__1down;1',\n",
       " b'650_300_NoSys;1',\n",
       " b'650_300_bkg_sig_NoSys;1']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events = uproot.open(save_path+'Wh_results_{}_to_{}.root'.format(nominal_signal_names[0],\n",
    "                                                                nominal_signal_names[-1]))\n",
    "# events = uproot.open('root_results/stored_results/275_50_train_injection_mod_independent/Wh_results_275_50_to_700_50.root')\n",
    "events.keys()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = '275_50'\n",
    "mbb_from_root_b = events['{}_bkg_sig_NoSys'.format(sig)].arrays(['mbb', 'weight', 'reco_loss'], \n",
    "                               outputtype=pd.DataFrame)\n",
    "mbb_from_root_bs = events['{}_NoSys'.format(sig)].arrays(['mbb', 'weight', 'reco_loss'], \n",
    "                               outputtype=pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "th = 7.15\n",
    "bins = 20\n",
    "\n",
    "f,a = plt.subplots(1,1, figsize=(8, 4))\n",
    "\n",
    "histo_range = (range_mbb[0], range_mbb[1])\n",
    "\n",
    "counts_b, edges_b = np.histogram((mbb_from_root_b[mbb_from_root_b['reco_loss']>th]['mbb']).astype(float)\n",
    "                                                    ,range = histo_range,\n",
    "                                 bins = bins, weights=mbb_from_root_b[mbb_from_root_b['reco_loss']>th]['weight'])\n",
    "\n",
    "a.step(x=edges_b, y=np.insert(counts_b,0,0),\n",
    "            color = 'black', label = 'only bkg')\n",
    "\n",
    "counts_bs, edges_bs = np.histogram((mbb_from_root_bs[mbb_from_root_bs['reco_loss']>th]['mbb']).astype(float)\n",
    "                                                    ,range = histo_range,\n",
    "                                 bins = bins,\n",
    "                                weights=mbb_from_root_bs[mbb_from_root_bs['reco_loss']>th]['weight'])\n",
    "\n",
    "a.hist((mbb_from_root_bs[mbb_from_root_bs['reco_loss']>th]['mbb']).astype(float), range = histo_range,\n",
    "                    bins = bins, weights=mbb_from_root_bs[mbb_from_root_bs['reco_loss']>th]['weight'],\n",
    "            color = 'lightsteelblue', label='bkg + sig')\n",
    "bincenters = 0.5*(edges_bs[1:]+edges_bs[:-1])\n",
    "a.set_ylim(bottom = 10**-1, top = 130)\n",
    "a.errorbar(bincenters, counts_bs, yerr=np.sqrt(counts_bs), fmt='.r', label='bkg + sig')\n",
    "#     a[0].set_yscale('log')\n",
    "a.legend(fontsize = 10)\n",
    "# a.yticks(20,25)\n",
    "a.set_ylabel('Events', fontsize=16)\n",
    "a.set_xlabel('mbb', fontsize=16)\n",
    "a.tick_params(labelsize='x-large')\n",
    "a.set_title('mbb distribution', fontsize = 14)\n",
    "\n",
    "plt.savefig('plot_results/root_bump_plot/{}/{}_{}.pdf'.format(type_mod,sig,th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41.52459733, 68.19992958, 94.00884773, 45.48545015])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_bs[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32.67405575, 45.42615339, 63.05482249, 37.84707086])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_b[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179.0021024939476"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(counts_b[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.85054158, 22.77377619, 30.95402524,  7.63837928])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_bs[:4] - counts_b[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.21672228926798"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model dependent\n",
    "np.sum(counts_bs[:4] - counts_b[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.37573757511325"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model independent\n",
    "np.sum(counts_bs[:4] - counts_b[:4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
